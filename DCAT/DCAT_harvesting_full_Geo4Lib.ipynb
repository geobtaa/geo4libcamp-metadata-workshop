{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Original created on Wed Mar 15 09:18:12 2017\n",
    "Edited Dec 28 2018; January 8, 2019; Dec 26-31, 2019, Jan 22 2020\n",
    "@author: kerni016\n",
    "\"\"\"\n",
    "## To run this script you need a csv with six columns (portalName, URL, provenance, isPartOf, publisher, and spatialCoverage) with details about ESRI open data portals to be checked for new records.\n",
    "## Need to define PreviousActionDate and ActionDate, directory path (containing newAll.csv and folders \"Jsons\" and \"Reports\"), and list of fields desired in the printed report\n",
    "## The script currently prints two combined reports - one of new items and one with deleted items.  Commented code allows the option to also print reports for each data portal.\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import urllib.request\n",
    "import os\n",
    "import os.path\n",
    "from html.parser import HTMLParser\n",
    "import decimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Manual items to change!\n",
    "\n",
    "## Set the date download of the older and newer jsons\n",
    "ActionDate = '20200122'\n",
    "PreviousActionDate = '20171106'\n",
    "\n",
    "##list of metadata fields from the DCAT json schema for open data portals desired in the final report\n",
    "fieldnames = [\"identifier\", \"code\", \"originalTitle\", \"title\", \"description\", \"subject\", \"keyword\", \"format\", \"type\", \"geometryType\", \"dateIssued\", \"temporalCoverage\", \"solrYear\", \"spatialCoverage\", \"spatial\", \"provenance\", \"publisher\",  \"creator\", \"landingPage\", \"downloadURL\", \"featureServer\", \"mapServer\", \"imageServer\"]\n",
    "\n",
    "##list of fields to use for the deletedItems report\n",
    "delFieldsReport = ['identifier', 'landingPage', 'portalName']\n",
    "\n",
    "##list of fields to use for the portal status report\n",
    "statusFieldsReport = ['portalName', 'total', 'new_items', 'deleted_items']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Removes html tags from text and replaces non-ascii characters with \"?\"\n",
    "###Code derived from Eloff on Stack Overflow : https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def cleanData (value):\n",
    "    fieldvalue = strip_tags(value)\n",
    "    fieldvalue = fieldvalue.encode('ascii', 'replace').decode('utf-8')\n",
    "    return fieldvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function that prints metadata elements from a dictionary to a csv file with as specified fields list as the header row\n",
    "def printReport (report, dictionary, fields):\n",
    "    with open(report, 'w', newline='') as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fields)\n",
    "        for keys in dictionary:\n",
    "            allvalues = dictionary[keys]\n",
    "            csvout.writerow(allvalues)\n",
    "\n",
    "### function that creates a dictionary with the position of a record in the data portal DCAT metadata json as the key and the identifier as the value\n",
    "def getIdentifiers (json):\n",
    "    json_ids = {}\n",
    "    for x in range(len(json[\"dataset\"])):\n",
    "        json_ids[x] = json[\"dataset\"][x][\"identifier\"]\n",
    "    return json_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###function that returns a dictionary of selected metadata elements for each new item in a data portal. This includes blank fields '' for columns that will be filled in manually later. Input requires the current DCAT json of the a data portal and a dictionary with key (position in the DCAT json), value (landing page URL) of the items. Includes an option to print a csv report of new items for each data portal\n",
    "def metadataNewItems(newdata, newitem_ids):\n",
    "    newItemDict = {}\n",
    "    ### key = position of the dataset in the DCAT metadata json, value = landing page URLs \n",
    "    for key, value in newitem_ids.items():\n",
    "        ###each time through the loop, creates an empty list to hold metadata about each item\n",
    "        metadata = []\n",
    "        \n",
    "        identifier = value\n",
    "        metadata.append(identifier.rsplit('/', 1)[-1])\n",
    "        metadata.append(portalName)\n",
    "        try:\n",
    "            metadata.append(cleanData(newdata[\"dataset\"][key]['title']))  #*\n",
    "        except:\n",
    "            metadata.append(newdata[\"dataset\"][key]['title'])\n",
    "\n",
    "        originalTitle = \"\"\n",
    "        metadata.append(originalTitle)\n",
    "        metadata.append(cleanData(newdata[\"dataset\"][key]['description'])) #*\n",
    "\n",
    "        ### Set default blank values for genre, format, type, and downloadURL\n",
    "        format_types = []\n",
    "        #genre = \"\"\n",
    "        formatElement = \"\"\n",
    "        typeElement = \"\"\n",
    "        downloadURL =  \"\"\n",
    "        geometryType = \"\"\n",
    "        webService = \"\"\n",
    "\n",
    "        distribution = newdata[\"dataset\"][key][\"distribution\"]\n",
    "        for dictionary in distribution:\n",
    "            try:\n",
    "                ### If one of the distributions is a shapefile, change genre/format and get the downloadURL\n",
    "                format_types.append(dictionary[\"title\"])\n",
    "                if dictionary[\"title\"] == \"Shapefile\":\n",
    "                    genre = \"Geospatial data\"\n",
    "                    formatElement = \"Shapefile\"\n",
    "                    if 'downloadURL' in dictionary.keys():\n",
    "                        downloadURL = dictionary[\"downloadURL\"]\n",
    "                    else:\n",
    "                        downloadURL = dictionary[\"accessURL\"]\n",
    "\n",
    "                    geometryType = \"Vector\"\n",
    "\n",
    "                ### If the Rest API is based on an ImageServer, change genre, type, and format to relate to imagery\n",
    "                if dictionary[\"title\"] == \"Esri Rest API\":\n",
    "                    #imageCheck = dictionary['accessURL'].rsplit('/', 1)[-1]\n",
    "                    if 'accessURL' in dictionary.keys():\n",
    "                        webService = dictionary['accessURL']\n",
    "                    \n",
    "                        if webService.rsplit('/', 1)[-1] == 'ImageServer':\n",
    "                            genre = \"Aerial imagery\"\n",
    "                            formatElement = 'Imagery'\n",
    "                            typeElement = 'Image|Service'\n",
    "                            #### Change this to Raster or Image?\n",
    "                            geometryType = \"\"\n",
    "\n",
    "                        ### If one of the distributions is a pdf, change genre and format\n",
    "                        elif \".pdf\" in webService:\n",
    "                            genre = 'Flagged'\n",
    "                            formatElement = 'PDF'\n",
    "                            typeElement = \"\"\n",
    "                            downloadURL =  \"\"\n",
    "\n",
    "                        ### If one of the distributions is a web application, change genre and format\n",
    "                        elif \"/apps/\" in webService:\n",
    "                            genre = 'Flagged'\n",
    "                            formatElement = 'Web application'\n",
    "                            typeElement = \"\"\n",
    "                            downloadURL =  \"\"\n",
    "                    else:\n",
    "                        genre = \"error - no download URL\"\n",
    "                        formatElement = \"error\"\n",
    "                        typeElement = \"\"\n",
    "                        downloadURL =  \"\"\n",
    "\n",
    "            ### If the distribution section of the metadata is not structured in a typical way\n",
    "            except:\n",
    "                ### Set default error values for genre, format, type, and downloadURL\n",
    "                genre = \"error - dictionary structure\"\n",
    "                formatElement = \"error\"\n",
    "                typeElement = \"error\"\n",
    "                downloadURL =  \"error\"\n",
    "\n",
    "                continue            \n",
    "          \n",
    "                \n",
    "        ###If the item has both a Shapefile and Esri Rest API format, change type\n",
    "        if \"Esri Rest API\" in format_types:\n",
    "            if \"Shapefile\" in format_types:\n",
    "                typeElement = \"Dataset|Service\"\n",
    "        ### If the distribution section is well structured but doesn't include either a shapefile or imagery, add a list of format types and set genre to 'flagged\n",
    "        if formatElement == \"\":\n",
    "            genre = 'flagged - other format'\n",
    "            formatElement = '|'.join(format_types)\n",
    "\n",
    "        ### Checks for patterns in spatial coordinates that frequently indicate an error and, if found, changes the genre to \"Suspicious coordinates\"\n",
    "\n",
    "        try:\n",
    "            bbox = []\n",
    "            spatial = cleanData(newdata[\"dataset\"][key]['spatial']) #*\n",
    "            typeDmal = decimal.Decimal\n",
    "            fix4 = typeDmal(\"0.0001\")\n",
    "            for coord in spatial.split(\",\"):\n",
    "                coordFix = typeDmal(coord).quantize(fix4)\n",
    "                bbox.append(str(coordFix))\n",
    "            count = 0\n",
    "            for coord in bbox:\n",
    "                if coord == '0.0000':\n",
    "                    count += 1\n",
    "            if count >= 2:\n",
    "                genre = 'flagged - suspicious coordinates'\n",
    "        except:\n",
    "            spatial = \"\"\n",
    "\n",
    "        #metadata.append(genre)\n",
    "        subject = \"\"\n",
    "        metadata.append(subject)\n",
    "        \n",
    "        keywords = newdata[\"dataset\"][key][\"keyword\"]\n",
    "        keyword_list = '|'.join(keywords)\n",
    "        metadata.append(keyword_list)\n",
    "        \n",
    "        metadata.append(formatElement)\n",
    "        metadata.append(typeElement)\n",
    "        metadata.append(geometryType)\n",
    "\n",
    "        metadata.append(cleanData(newdata[\"dataset\"][key]['issued'])) #*\n",
    "        temporalCoverage = \"\"\n",
    "        metadata.append (temporalCoverage)\n",
    "        dateElement = \"\"\n",
    "        metadata.append(dateElement)\n",
    "        metadata.append(spatialCoverage)\n",
    "        metadata.append(spatial)\n",
    "\n",
    "        metadata.append(provenance)\n",
    "#       metadata.append(isPartOf)\n",
    "        metadata.append(publisher)\n",
    "        \n",
    "        creator = newdata[\"dataset\"][key][\"publisher\"]\n",
    "        for pub in creator.values():\n",
    "            creator = pub.encode('ascii', 'replace').decode('utf-8')\n",
    "#       creator = creator['source'].encode('ascii', 'replace')\n",
    "        metadata.append(creator)\n",
    "\n",
    "        metadata.append(cleanData(newdata[\"dataset\"][key]['landingPage'])) #*\n",
    "        metadata.append(downloadURL)\n",
    "        #metadata.append(webService)\n",
    "\n",
    "#       webService = cleanData(newdata[\"dataset\"][key]['webService'])  #*\n",
    "\n",
    "        featureServer = \"\"\n",
    "        mapServer = \"\"\n",
    "        imageServer = \"\"\n",
    "        \n",
    "        try:\n",
    "            if \"FeatureServer\" in webService:\n",
    "                featureServer = webService\n",
    "            if \"MapServer\" in webService:\n",
    "                mapServer = webService\n",
    "            if \"ImageServer\" in webService:\n",
    "                imageServer = webService\n",
    "        except:\n",
    "                print(identifier)\n",
    "        \n",
    "        metadata.append(featureServer)\n",
    "        metadata.append(mapServer)\n",
    "        metadata.append(imageServer)\n",
    "\n",
    "        newItemDict[identifier] = metadata\n",
    "        \n",
    "    return newItemDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05b-27053 http://gis-hennepin.opendata.arcgis.com/data.json\n",
      "05b-27143 http://stlouiscountymndata-slcgis.opendata.arcgis.com/data.json\n",
      "05b-27163 https://data-wcmn.opendata.arcgis.com/data.json\n",
      "06a-01 http://gis-michigan.opendata.arcgis.com/data.json\n"
     ]
    }
   ],
   "source": [
    "### Sets up lists to hold metadata information from each portal to be printed to a report\n",
    "All_New_Items = []\n",
    "All_Deleted_Items = []\n",
    "Status_Report = {}\n",
    "\n",
    "### Opens a list of portals and urls ending in /data.json from input CSV using column headers 'portalName' and 'URL'\n",
    "with open('PortalList.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        ### Read in values from the portals list to be used within the script or as part of the metadata report\n",
    "        portalName = row['portalName']\n",
    "        url = row['URL']\n",
    "        provenance = row['provenance']\n",
    "        publisher = row['publisher']\n",
    "        spatialCoverage = row['spatialCoverage']\n",
    "        print (portalName, url)\n",
    "\n",
    "        ## for each open data portal in the csv list...\n",
    "        ## renames file paths based on portalName and manually provided dates\n",
    "        oldjson = 'DCATjsons/%s_%s.json' % (portalName, PreviousActionDate)\n",
    "        newjson = 'DCATjsons/%s_%s.json' % (portalName, ActionDate)\n",
    "\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            newdata = json.load(response)\n",
    "        \n",
    "        except:\n",
    "            print (\"Data portal URL does not exist: \" + url)\n",
    "            break\n",
    "\n",
    "            \n",
    "        ### Saves a copy of the json to be used for the next round of comparison/reporting\n",
    "        with open(newjson, 'w') as outfile:\n",
    "            json.dump(newdata, outfile)\n",
    "\n",
    "            ### collects information about number of resources (total, new, and old) in each portal\n",
    "            status_metadata = []\n",
    "            status_metadata.append(portalName)\n",
    "\n",
    "        #Opens older copy of data/json downloaded from the specified Esri Open Data Portal.  If this file does not exist, treats every item in the portal as new\n",
    "        if os.path.exists(oldjson):\n",
    "            with open(oldjson) as data_file:\n",
    "                older_data = json.load(data_file)\n",
    "\n",
    "            ### Makes a list of dataset identifiers in the older json\n",
    "            older_ids = getIdentifiers (older_data)\n",
    "\n",
    "            ###compares identifiers in the older json harvest of the data portal with identifiers in the new json, creating dictionaries with 1) a complete list of new json identifiers and 2) a list of just the items that appear in the new json but not the older one\n",
    "            newjson_ids = {}\n",
    "            newitem_ids = {}\n",
    "\n",
    "            for y in range(len(newdata[\"dataset\"])):\n",
    "                identifier = newdata[\"dataset\"][y][\"identifier\"]\n",
    "                newjson_ids[y] = identifier\n",
    "                if identifier not in older_ids.values():\n",
    "                    newitem_ids[y] = identifier\n",
    "\n",
    "\n",
    "            ### creates a dictionary of metadata elements for each new data portal item. Includes an option to print a csv report of new items for each data portal\n",
    "            ### Puts dictionary of identifiers (key), metadata elements (values) for each data portal into a list (to be used printing the combined report) [portal1{identifier:[metadataElement1, metadataElement2, ... ], portal2{identifier:[metadataElement1, metadataElement2, ... ], ...}\n",
    "            All_New_Items.append(metadataNewItems(newdata, newitem_ids))\n",
    "\n",
    "            ### collects information for the status report about the number of records currently in the portal and new items\n",
    "            status_metadata.append(len(newjson_ids))\n",
    "            status_metadata.append(len(newitem_ids))\n",
    "\n",
    "            ### Compares identifiers in the older json to the list of identifiers from the newer json. If the record no longer exists, adds selected fields into a dictionary of deleted items (deletedItemDict)\n",
    "            deletedItemDict = {}\n",
    "            for z in range(len(older_data[\"dataset\"])):\n",
    "                identifier = older_data[\"dataset\"][z][\"identifier\"]\n",
    "                if identifier not in newjson_ids.values():\n",
    "                    del_metadata = []\n",
    "                    del_metadata.append(identifier.rsplit('/', 1)[-1])\n",
    "                    del_metadata.append(identifier)\n",
    "                    del_metadata.append(portalName)\n",
    "                    deletedItemDict[identifier] = del_metadata\n",
    "\n",
    "            ### Puts dictionary of identifiers (key), metadata elements (values) for each data portal into a list (to be used printing the combined report) [portal1{identifier:[metadataElement1, metadataElement2, ... ], portal2{identifier:[metadataElement1, metadataElement2, ... ], ...}\n",
    "            All_Deleted_Items.append(deletedItemDict)\n",
    "\n",
    "            ### collects information for the status report about the number of deleted items\n",
    "            status_metadata.append(len(deletedItemDict))\n",
    "            Status_Report [portalName] = status_metadata\n",
    "\n",
    "        ### if there is no older json for comparions....\n",
    "        else:\n",
    "            print (\"There is no comparison json for %s\" % (portalName))\n",
    "            ### Makes a list of dataset identifiers in the new json\n",
    "            newjson_ids = getIdentifiers (newdata)\n",
    "\n",
    "            ### creates a dictionary of metadata elements for each new item in a data portal (i.e. all items from the new json). Includes an option to print a csv report of new items for each data portal\n",
    "            ### Puts dictionary of identifiers (key), metadata elements (values) for each data portal into a list (to be used printing the combined report)   [portal1{identifier:[metadataElement1, metadataElement2, ... ], portal2{identifier:[metadataElement1, metadataElement2, ... ], ...}\n",
    "            All_New_Items.append(metadataNewItems(newdata, newjson_ids))\n",
    "\n",
    "            ### collects information for the status report about the number of records currently in the portal, new items, and deleted items\n",
    "            status_metadata.append(len(newjson_ids))\n",
    "            status_metadata.append(len(newjson_ids))\n",
    "            status_metadata.append('0')\n",
    "            Status_Report [portalName] = status_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prints two csv spreadsheets with all items that are new or deleted since the last time the data portals were harvested\n",
    "report = \"NewDCATitems_full_%s.csv\" %  (ActionDate)\n",
    "with open(report, 'w', newline='', encoding=\"utf-8\") as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fieldnames)\n",
    "        for portal in All_New_Items:\n",
    "            for keys in portal:\n",
    "                allvalues = portal[keys]\n",
    "                csvout.writerow(allvalues)\n",
    "\n",
    "report = \"DeletedDCATitems_full_%s.csv\" %  (ActionDate)\n",
    "with open(report, 'w', newline='', encoding=\"utf-8\") as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(delFieldsReport)\n",
    "        for portal in All_Deleted_Items:\n",
    "            for keys in portal:\n",
    "                allvalues = portal[keys]\n",
    "                csvout.writerow(allvalues)\n",
    "\n",
    "### prints a status report about how many items have been added or deleted from each portal.\n",
    "reportStatus = \"portal_status_report_%s.csv\" % (ActionDate)\n",
    "printReport (reportStatus, Status_Report, statusFieldsReport)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
